{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to be run from the host machine to submit jobs to Kubernetes.\n",
    "\n",
    "Before running this notebook, download the raw Spotify Million Playlist Challenge data from AI Crowd:\n",
    "- Download the 2 data folders from [AI Crowd's Spotify contest page](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge/dataset_files).\n",
    "- Unzip them, and place them in `kube-transform/data/spotify_mpc/raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'spotify_mpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.extend([\n",
    "    \"../../\",\n",
    "    \"../../execution\",\n",
    "    \"../../orchestration\",\n",
    "])\n",
    "\n",
    "import os\n",
    "from orchestration.spotify_mpc import orchestrate as orch\n",
    "from orchestration.submit import submit_job\n",
    "\n",
    "os.environ['PROJECT_NAME'] = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Local\n",
    "os.environ['K8S_ENV'] = 'minikube'\n",
    "os.environ['DATA_DIR'] = '/'.join(os.getcwd().split(\"/\")[:-2] + ['data'])\n",
    "! ../../build_scripts/build_local.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run `standardize_data`.\n",
    "\n",
    "This function:\n",
    "- Groups the raw data into 100 batches of 10k playlists each.\n",
    "- Saves each part as:\n",
    "    - A thin playlist dataframe containing 1 row per playlist.\n",
    "    - A track df with information about each unique track found in those playlists.\n",
    "\n",
    "The output will be stored in the /standardized folder in the data directory.\n",
    "\n",
    "This function takes in a list of \"parts\", where a part represents one of the 100 batches.  For this small contest, let's just standardize the first 8 batches.\n",
    "\n",
    "orch.standardize_data will therefore produce a Job Config that specifies 8 tasks - one for each part that we want to standardize.\n",
    "\n",
    "These tasks will get scheduled on our k8s cluster. If we're running this locally, some may need to wait for others to finish. If we're running on EKS, they should all execute in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_job(\n",
    "    orch.standardize_data(\n",
    "        input_directory=\"spotify_mpc/raw/spotify_million_playlist_dataset/data\",\n",
    "        output_directory=\"spotify_mpc/standardized\",\n",
    "        parts=list(range(8)) # 0-5 for features, 6 for training, 7 for testing\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a contest from this standardized data.\n",
    "\n",
    "The contest will consist of:\n",
    "- A train directory containing 6 training files.\n",
    "- A track dataframe containing all of the unique tracks present in those 60k training playlists.\n",
    "- A challenge_set file containing the challenge set that we'll test ourselves against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_job(\n",
    "    orch.create_contest(\n",
    "        standardized_input_directory=\"spotify_mpc/standardized\",\n",
    "        train_parts=list(range(6)),\n",
    "        track_df_output_path=\"spotify_mpc/80KPC/track_df.parquet\",\n",
    "        train_output_directory=\"spotify_mpc/80KPC/train\",\n",
    "        test_part=7,\n",
    "        n_test_cases=5000,\n",
    "        challenge_set_output_path=\"spotify_mpc/80KPC/challenge_set.parquet\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create another challenge set to use for trainig.\n",
    "\n",
    "Our approach will be:\n",
    "- Use parts 0-5 (60k playlists) for feature generation.\n",
    "- Use a challenge set derived from part 6 for training and validation (i.e. to train the model that maps features to the probability that a given track is on a given playlist).\n",
    "- Use a challenge set derived from part 7 for evaluation against Spotify's metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_job(\n",
    "    orch.create_challenge_set(\n",
    "        standardized_input_directory=\"spotify_mpc/standardized\",\n",
    "        test_part=6,\n",
    "        track_df_path=\"spotify_mpc/80KPC/track_df.parquet\",\n",
    "        n_test_cases=10000,\n",
    "        output_path=\"spotify_mpc/80KPC/challenge_set_training.parquet\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll generate co-occurrence dictionaries.\n",
    "\n",
    "A given task will handle a single training part (10k playlists). During a task, we'll:\n",
    "* Count the co-occurrences between every pair of tracks, and represent using a nested dictionary (sparse representation)\n",
    "    * Each key will be a track ID, and each value will be a dict of track ID to co-occurrence count.\n",
    "* Split the resulting dictionary into 10 shards, using a hashing algorithm to ensure that a given key will always map to a given shard.\n",
    "* Do this for several different kinds of co-occurrence type: total, forward (i.e. A preceeds B in the playlist), etc.\n",
    "\n",
    "Notes:\n",
    "* Each task handles a fixed number of playlists (10k), regardless of the overall contest scale.\n",
    "* The output will only contain top-level keys from track IDs that are contained in the seed tracks our challenge DFs. This is an optimization to reduce computation, because we won't need features for other track IDs.\n",
    "* The output from a given task is split into a configurable number of shards (10), for scalable aggregation later.\n",
    "* The output is not \"complete\": any given dictionary contains incomplete information for its keys - only the co-occurrence counts found in a subset of playlists.\n",
    "* This may take a while (~12 minutes) when running locally, even for the 80KPC contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_job(\n",
    "    orch.generate_co_dicts(\n",
    "        train_directory=\"spotify_mpc/80KPC/train\",\n",
    "        challenge_df_paths=[f\"spotify_mpc/80KPC/challenge_set{suffix}.parquet\" for suffix in [\"\", \"_training\"]],\n",
    "        partial_co_dict_output_directory=\"spotify_mpc/80KPC/pco\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll aggregate the co-occurrence dictionaries.\n",
    "\n",
    "A given task will handle a single shard (1/10 of the tracks found in our challenge DFs) for a single co-occurrence type. During a task, we'll:\n",
    "* Load the co-occurrence dict for the target shard from each training part.\n",
    "* Sum these counts to create a complete co-occurrence dict for the tracks in the shard.\n",
    "\n",
    "Notes:\n",
    "* Each task handles 1/10 of the total track IDs found in our challenge DF seed tracks.\n",
    "* While we never hold all co-occurrence information in memory at once - or within a single a file - the output from each task is complete in that, for the tracks in the shard, it contains complete co-occurrence info across the entire training set.\n",
    "* This may also take (~12 minutes) when running locally for the 80KPC contest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_job(\n",
    "    orch.reduce_co_partials(\n",
    "        partial_co_dict_directory=\"spotify_mpc/80KPC/pco\",\n",
    "        co_dict_output_directory=\"spotify_mpc/80KPC/co\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's identify playlists that match an artist's name, and aren't common phrases.\n",
    "\n",
    "This will create a useful feature for our deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in [\"\", \"_training\"]:\n",
    "    submit_job(\n",
    "        orch.identify_artist_playlists(\n",
    "            challenge_df_path=f\"spotify_mpc/80KPC/challenge_set{suffix}.parquet\",\n",
    "            track_df_path=\"spotify_mpc/80KPC/track_df.parquet\",\n",
    "            output_path=f\"spotify_mpc/80KPC/artist_pids{suffix}.json\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll generate generic features for our deep learning model.\n",
    "\n",
    "These features are \"generic\" in that they pertain to a playlist in the challenge set in general, not to a particular <playlist, candidate track> pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in [\"\", \"_training\"]:\n",
    "    submit_job(\n",
    "        orch.generate_generic_features_fcnn_mfe(\n",
    "            challenge_df_path=f\"spotify_mpc/80KPC/challenge_set{suffix}.parquet\", #\n",
    "            track_df_path=\"spotify_mpc/80KPC/track_df.parquet\",\n",
    "            artist_playlists_path=f\"spotify_mpc/80KPC/artist_pids{suffix}.json\", #\n",
    "            co_dict_directory=\"spotify_mpc/80KPC/co\",\n",
    "            output_directory=f\"spotify_mpc/80KPC/generic_features_fcnn{suffix}\", #\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next transformation does the heavy lifting. It will generate the data samples that we'll use to train our model.\n",
    "\n",
    "It will create a task for each 1000-row subset of the challenge set. For each row, it will:\n",
    "- Identify several thousand candidate tracks for the playlist using heuristics.\n",
    "- Create a full set of features for each <playlist, candidate track> pair.\n",
    "\n",
    "It will do this for both:\n",
    "- the \"training\" challenge set, which we'll use to train a model that can map these features into the probability that a candidate track will be present on a playlist.\n",
    "- the \"real\" challenge set, which we'll use to evaluate our model.  We'll infer against these features, and then evaluate those predictions using Spotify's evaluation metrics.\n",
    "\n",
    "This can take around 35 minutes locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in [\"\", \"_training\"]:\n",
    "    submit_job(\n",
    "        orch.generate_track_features_fcnn_mfe(\n",
    "            challenge_df_path=f\"spotify_mpc/80KPC/challenge_set{suffix}.parquet\", #\n",
    "            track_df_path=\"spotify_mpc/80KPC/track_df.parquet\",\n",
    "            challenge_df_generic_features_directory=f\"spotify_mpc/80KPC/generic_features_fcnn{suffix}\", #\n",
    "            co_dict_directory=\"spotify_mpc/80KPC/co\",\n",
    "            output_directory=f\"spotify_mpc/80KPC/samples_fcnn{suffix}\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training samples, we need to train our model.\n",
    "\n",
    "For this step, open a high-memory Google Colab instance (CPU is fine for this contest) or any cloud compute instance. You can try running locally instead, but you'll need to install some dependencies first:\n",
    "`pip install tensorflow pandas numpy`\n",
    "\n",
    "If you're not running locally, zip your sample data ('samples_fcnn' and 'samples_fcnn_training') and upload it to your Google drive (for Colab) or your compute instance.\n",
    "\n",
    "Open the colab/spotify-fcnn.ipynb notebook, make sure set the TRAINING_DIRS and TEST_DIRS to point to your sample data, and run the cells.  It should generate a submission parquet file. This holds the 500 top track predictions for each playlist in the challenge set.\n",
    "\n",
    "Download that submission file, place it in your data directory, and then proceed to the next transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_job(\n",
    "    orch.evaluate_submission(\n",
    "        challenge_df_path=\"spotify_mpc/80KPC/challenge_set.parquet\",\n",
    "        track_df_path=\"spotify_mpc/80KPC/track_df.parquet\",\n",
    "        submission_directory=\"spotify_mpc/80KPC/submission_fcnn\",\n",
    "        output_directory=\"spotify_mpc/80KPC/evaluation_fcnn\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running in EKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Remote\n",
    "import boto3\n",
    "os.environ['AWS_ACCOUNT_ID'] = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "os.environ['K8S_ENV'] = 'eks'\n",
    "os.environ['DATA_DIR'] = 's3://kube-transform-data-bucket'\n",
    "! ../../build_scripts/build_eks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw data is large and can take a while to upload to S3. I'd recommend running the standardization job locally first.\n",
    "# Then, upload the standardized data to S3, build the remote image, and run the rest of the jobs in EKS.\n",
    "\n",
    "! aws s3 cp ../../data/spotify_mpc/standardized s3://kube-transform-data-bucket/spotify_mpc/standardized --recursive\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
